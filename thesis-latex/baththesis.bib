@online{Gualtieri,
title={Hadoop Is Data's Darling For A Reason},
url={https://go.forrester.com/blogs/hadoop-is-datas-darling-for-a-reason/},
journal={Forrester},
author={Gualtieri, Mike},
year={2020}
}

@article{ShangZeyuan2019DDSt,
series = {the 2019 International Conference},
issn = {0730-8078},
pages = {1171--1188},
publisher = {ACM Press},
booktitle = {Proceedings of the 2019 International Conference on Management of Data - SIGMOD '19},
isbn = {9781450356435},
year = {2019},
title = {Democratizing Data Science through Interactive Curation of ML Pipelines},
author = {Shang, Zeyuan and Zgraggen, Emanuel and Buratti, Benedetto and Kossmann, Ferdinand and Eichmann, Philipp and Chung, Yeounoh and Binnig, Carsten and Upfal, Eli and Kraska, Tim},
}

@article{KraskaT.2018NAid,
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
pages = {2150--2164},
volume = {11},
publisher = {Association for Computing Machinery},
number = {12},
year = {2018},
title = {Northstar: An interactive data science system},
copyright = {Copyright 2019 Elsevier B.V., All rights reserved.},
author = {Kraska, T.},
}

@online{powers,
title={Powers of 10: time scales in used experience},
url={https://www.nngroup.com/articles/powers-of-10-time-scales-in-ux/},
journal={Nielsen Norman Group},
year={2020}
}

@book{NewmanSamuel2015Bm:d,
publisher = {O'Reilly},
isbn = {1-4919-5033-1},
year = {2015},
title = {Building microservices : designing fine-grained systems},
edition = {First edition.},
language = {eng},
address = {Sebastopol, California},
author = {Newman, Samuel},
keywords = {Computer architecture; Distributed operating systems (Computers); Computer software -- Development; Electronic books},
}

@book{FehlingChristoph2014CCP:,
abstract = {The current work provides CIOs, software architects, project managers, developers, and cloud strategy initiatives with a set of architectural patterns that offer nuggets of advice on how to achieve common cloud computing-related goals. The cloud computing patterns capture knowledge and experience in an abstract format that is independent of concrete vendor products. Readers are provided with a toolbox to structure cloud computing strategies and design cloud application architectures. By using this book cloud-native applications can be implemented and best suited cloud vendors and tooling for individual usage scenarios can be selected. The cloud computing patterns offer a unique blend of academic knowledge and practical experience due to the mix of authors. Academic knowledge is brought in by Christoph Fehling and Professor Dr. Frank Leymann who work on cloud research at the University of Stuttgart. Practical experience in building cloud applications, selecting cloud vendors, and designing enterprise architecture as a cloud customer is brought in by Dr. Ralph Retter who works as an IT architect at T‑Systems, Walter Schupeck, who works as a Technology Manager in the field of Enterprise Architecture at Daimler AG,and Peter Arbitter, the former head of T Systems’ cloud architecture and IT portfolio team and now working for Microsoft. Voices on Cloud Computing Patterns Cloud computing is especially beneficial for large companies such as Daimler AG. Prerequisite is a thorough analysis of its impact on the existing applications and the IT architectures. During our collaborative research with the University of Stuttgart, we identified a vendor-neutral and structured approach to describe properties of cloud offerings and requirements on cloud environments. The resulting Cloud Computing Patterns have profoundly impacted our corporate IT strategy regarding the adoption of cloud computing. They help our architects, project managers and developers in the refinement of architectural guidelines and communicate requirements to our integration partners and software suppliers.  Dr. Michael Gorriz – CIO Daimler AG  Ever since 2005 T-Systems has provided a flexible and reliable cloud platform with its “Dynamic Services”. Today these cloud services cover a huge variety of corporate applications, especially enterprise resource planning, business intelligence, video, voice communication, collaboration, messaging and mobility services. The book was written by senior cloud pioneers sharing their technology foresight combining essential information and practical experiences. This valuable compilation helps both practitioners and clients to really understand which new types of services are readily available, how they really work and importantly how to benefit from the cloud.   Dr. Marcus Hacke – Senior Vice President, T-Systems International GmbH This book provides a conceptual framework and very timely guidance for people and organizations building applications for the cloud. Patterns are a proven approach to building robust and sustainable applications and systems. The authors adapt and extend it to cloud computing, drawing on their own experience and deep contributions to the field. Each pattern includes an extensive discussion of the state of the art, with implementation considerations and practical examples that the reader can apply to their own projects. By capturing our collective knowledge about building good cloud applications and by providing a format to integrate new insights, this book provides an important tool not just for individual practitioners and teams, but for the cloud computing community at large.  Kristof Kloeckner – General Manager,Rational Software, IBMSoftware Group  .},
publisher = {Springer Vienna : Imprint: Springer},
isbn = {3-7091-1568-X},
year = {2014},
title = {Cloud Computing Patterns : Fundamentals to Design, Build, and Manage Cloud Applications},
edition = {1st ed. 2014.},
language = {eng},
address = {Vienna},
author = {Fehling, Christoph},
keywords = {Computer system performance; Computer science; Operating systems (Computers); System Performance and Evaluation; Models and Principles; Performance and Reliability},
}

@online{whatisacontainer,
title={What is a container?},
url={https://www.docker.com/resources/what-container}, journal={Docker},
year={2020}
}

@inproceedings{KangHui2016CaMD,
series = {2016 IEEE International Conference on Cloud Engineering (IC2E)},
pages = {202--211},
publisher = {IEEE},
booktitle = {2016 IEEE International Conference on Cloud Engineering (IC2E)},
isbn = {1-5090-1962-6},
year = {2016},
title = {Container and Microservice Driven Design for Cloud Infrastructure DevOps},
author = {Kang, Hui and Le, Michael and Tao, Shu},
}

@article{pymfe,
author = {Rivolli, Adriano and Garcia, Luís Paulo and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, Andre},
year = {2018},
month = {08},
pages = {},
title = {Towards Reproducible Empirical Research in Meta-Learning}
}

@online{totaljs,
title={TotalJS},
url={https://github.com/totaljs/framework},
journal={GitHub},
year={2020}
}

@online{retejs,
title={ReteJS},
url={https://github.com/retejs/rete},
journal={GitHub},
year={2020}
}

@online{stormreact,
title={StormReact},
url={https://github.com/projectstorm/react-diagrams},
journal={GitHub},
year={2020}
}

@online{taverna,
title={Apache Taverna},
url={https://taverna.incubator.apache.org/},
year={2018}
}

@online{yawl,
title={YAWL-Yet Another Workflow Language},
url={https://yawlfoundation.github.io/},
year={2020}
}

@article{Milicchio2016,
abstract = {Background: High-throughput or next-generation sequencing (NGS) technologies have become an established and affordable experimental framework in biological and medical sciences for all basic and translational research. Processing and analyzing NGS data is challenging. NGS data are big, heterogeneous, sparse, and error prone. Although a plethora of tools for NGS data analysis has emerged in the past decade, (i) software development is still lagging behind data generation capabilities, and (ii) there is a 'cultural' gap between the end user and the developer. Text: Generic software template libraries specifically developed for NGS can help in dealing with the former problem, whilst coupling template libraries with visual programming may help with the latter. Here we scrutinize the state-of-the-art low-level software libraries implemented specifically for NGS and graphical tools for NGS analytics. An ideal developing environment for NGS should be modular (with a native library interface), scalable in computational methods (i.e. serial, multithread, distributed), transparent (platform-independent), interoperable (with external software interface), and usable (via an intuitive graphical user interface). These characteristics should facilitate both the run of standardized NGS pipelines and the development of new workflows based on technological advancements or users' needs. We discuss in detail the potential of a computational framework blending generic template programming and visual programming that addresses all of the current limitations. Conclusion: In the long term, a proper, well-developed (although not necessarily unique) software framework will bridge the current gap between data generation and hypothesis testing. This will eventually facilitate the development of novel diagnostic tools embedded in routine healthcare.},
author = {Milicchio, Franco and Rose, Rebecca and Bian, Jiang and Min, Jae and Prosperi, Mattia},
doi = {10.1186/s13040-016-0095-3},
file = {:home/robert/Downloads/Visual programming for next-generation sequencing data analytics.pdf:pdf},
issn = {17560381},
journal = {BioData Mining},
keywords = {Big data,Generic programming,Graphical user interface,High-throughput sequencing,Next-generation sequencing,Software suite,Template library,Visual programming},
number = {1},
pages = {1--18},
publisher = {BioData Mining},
title = {{Visual programming for next-generation sequencing data analytics}},
url = {http://dx.doi.org/10.1186/s13040-016-0095-3},
volume = {9},
year = {2016}
}
@article{Mills2016,
abstract = {The term data scientist has only been in common use since 2008, but in 2016 it is considered one of the top careers in the United States. The purpose of this paper is to explore the growth of data science content areas such as analytics, business intelligence, and big data in AACSB Information Systems (IS) programs between 2011 and 2016. A secondary purpose is to analyze the effect of IS programs' adherence to IS 2010 Model Curriculum Guidelines for undergraduate MIS programs, as well as the impact of IS programs offering an advanced database course in 2011 on data science course offerings in 2016. A majority (60{\%}) of AACSB IS programs added data science-related courses between 2011 and 2016. Results indicate dramatic increases in courses offered in big data analytics (583{\%}), visualization (300{\%}), business data analysis (260{\%}), and business intelligence (236{\%}). ANOVA results also find a significant effect of departments offering advanced database courses in 2011 on new analytics course offerings in 2016. A Chi-Square analysis did not find an effect of IS 2010 Model Curriculum adherence on analytics course offerings in 2016. Implications of our findings for an MIS department's ability to respond to changing needs of the marketplace and its students are discussed.},
author = {Mills, Robert J. and Chudoba, Katherine M. and Olsen, David H.},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Programs Responding to Industry Demands for Data
Scientists$\backslash$:.pdf:pdf},
issn = {10553096},
journal = {Journal of Information Systems Education},
keywords = {Big data,Business intelligence,Data analytics,Model curricula,Visualization},
number = {2},
pages = {131--140},
title = {{Is programs responding to industry demands for data scientists: A comparison between 2011 - 2016}},
volume = {27},
year = {2016}
}
@book{Cavanillas2016,
abstract = {In this book readers will find technological discussions on the existing and emerging technologies across the different stages of the big data value chain. They will learn about legal aspects of big data, the social impact, and about education needs and requirements. And they will discover the business perspective and how big data technology can be exploited to deliver value within different sectors of the economy. The book is structured in four parts: Part I "The Big Data Opportunity" explores the value potential of big data with a particular focus on the European context. It also describes the legal, business and social dimensions that need to be addressed, and briefly introduces the European Commission's BIG project. Part II "The Big Data Value Chain" details the complete big data lifecycle from a technical point of view, ranging from data acquisition, analysis, curation and storage, to data usage and exploitation. Next, Part III "Usage and Exploitation of Big Data" illustrates the value creation possibilities of big data applications in various sectors, including industry, healthcare, finance, energy, media and public services. Finally, Part IV "A Roadmap for Big Data Research" identifies and prioritizes the cross-sectorial requirements for big data research, and outlines the most urgent and challenging technological, economic, political and societal issues for big data in Europe. This compendium summarizes more than two years of work performed by a leading group of major European research centers and industries in the context of the BIG project. It brings together research findings, forecasts and estimates related to this challenging technological context that is becoming the major axis of the new digitally transformed business environment.},
author = {Cavanillas, Jos{\'{e}} Mar{\'{i}}a and Curry, Edward and Wahlster, Wolfgang},
booktitle = {New Horizons for a Data-Driven Economy: A Roadmap for Usage and Exploitation of Big Data in Europe},
doi = {10.1007/978-3-319-21569-3},
file = {:home/robert/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cavanillas, Curry, Wahlster - 2016 - New Horizons for a Data-Driven Economy A Roadmap for Usage and Exploitation of Big Data in Europe.pdf:pdf},
isbn = {9783319215693},
month = {jan},
pages = {1--303},
publisher = {Springer International Publishing},
title = {{New Horizons for a Data-Driven Economy: A Roadmap for Usage and Exploitation of Big Data in Europe}},
year = {2016}
}
@article{Thones2015,
abstract = {In this excerpt from Software Engineering Radio, Johannes Th{\"{o}}nes talks with James Lewis, principal consultant at ThoughtWorks, about microservices. They discuss microservices' recent popularity, architectural styles, deployment, size, technical decisions, and consumer-driven contracts. They also compare microservices to service-oriented architecture and wrap up the episode by talking about key figures in the microservice community and standing on the shoulders of giants. The Web extra at http://www.se-radio.net/2014/10/episode-213-james-lewis-on-microservices is an audio recording of Tobias Kaatz speaking with James Lewis, principal consultant at ThoughtWorks, about microservices. They discuss microservices' recent popularity, architectural styles, deployment, size, technical decisions, and consumer-driven contracts. They also compare microservices to service-oriented architecture and wrap up the episode by talking about key figures in the microservice community and standing on the shoulders of giants.},
author = {Th{\"{o}}nes, Johannes},
doi = {10.1109/MS.2015.11},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Microservices.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {SE Radio,architecture,enterprise service bus,http,microservice,service-oriented architecture,software engineering},
number = {1},
title = {{Microservices}},
volume = {32},
year = {2015}
}
@article{Jaramillo2016,
abstract = {Microservices architecture is not a hype and for awhile, started getting attention from organizations who want to shorten time to market of a software product by improving productivity effect through maximizing the automation in all life circle of the product. However, microservices architecture approach also introduces a lot of new complexity and requires application developers a certain level of maturity in order to confidently apply the architectural style. Docker has been a disruptive technology which changes the way applications are being developed and distributed. With a lot of advantages, Docker is a very good fit to implementing microservices architecture. In this paper we will discuss about how Docker can effectively help in leveraging mircoservices architecture with a real working model as a case study.},
author = {Jaramillo, David and Nguyen, Duy V. and Smart, Robert},
doi = {10.1109/SECON.2016.7506647},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Leveraging microservices using docker.pdf:pdf},
isbn = {9781509022465},
issn = {07347502},
journal = {Conference Proceedings - IEEE SOUTHEASTCON},
keywords = {automation,devops,docker,micoservices},
pages = {1--5},
publisher = {IEEE},
title = {{Leveraging microservices architecture by using Docker technology}},
volume = {2016-July},
year = {2016}
}
@inproceedings{Kraska2018,
abstract = {In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the “guts.“ Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the “protection“ we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.},
author = {Kraska, Tim},
booktitle = {Proceedings of the VLDB Endowment},
doi = {10.14778/3229863.3240493},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Northstar$\backslash$: an interactive data science system
.pdf:pdf},
issn = {21508097},
number = {12},
pages = {2150--2164},
publisher = {Association for Computing Machinery},
title = {{Northstar: An interactive data science system}},
volume = {11},
year = {2018}
}
@article{Medvedev2016,
abstract = {SciServer Compute uses Jupyter notebooks running within server-side Docker containers attached to large relational databases and file storage to bring advanced analysis capabilities close to the data. SciServer Compute is a component of SciServer, a big-data infrastructure project developed at Johns Hopkins University that will provide a common environment for computational research. SciServer Compute integrates with large existing databases in the fields of astronomy, cosmology, turbulence, genomics, oceanography and materials science. These are accessible through the CasJobs service for direct SQL queries. SciServer Compute adds interactive server-side computational capabilities through notebooks in Python, R and MATLAB, an API for running asynchronous tasks, and a very large (hundreds of terabytes) scratch space for storing intermediate results. Science-ready results can be stored on a Dropbox-like service, SciDrive, for sharing with collaborators and dissemination to the public. Notebooks and batch jobs run inside Docker containers owned by the users. This provides security and isolation and allows flexible configuration of computational contexts through domain specific images and mounting of domain specific data sets. We present a demo that illustrates the capabilities of SciServer Compute: using Jupyter notebooks, performing analyses on data selections from diverse scientific fields, and running asynchronous jobs in a Docker container. The demo will highlight the data flow between file storage, database, and compute components.},
author = {Medvedev, Dmitry and Lemson, Gerard and Rippin, Mike},
doi = {10.1145/2949689.2949700},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Bringing analysis close to the data.pdf:pdf},
isbn = {9781450342155},
journal = {ACM International Conference Proceeding Series},
keywords = {Docker,Jupyter,Python,application containers,applied computing,data analysis,scientific workflows,virtualization},
title = {{SciServer compute: Bringing analysis close to the data}},
volume = {18-20-July},
year = {2016}
}
@article{VanDerAalst2005,
abstract = {Based on a rigorous analysis of existing workflow management systems and workflow languages, a new workflow language is proposed: yet another workflow language (YAWL). To identify the differences between the various languages, we have collected a fairly complete set of workflow patterns. Based on these patterns we have evaluated several workflow products and detected considerable differences in their ability to capture control flows for non-trivial workflow processes. Languages based on Petri nets perform better when it comes to state-based workflow patterns. However, some patterns (e.g. involving multiple instances, complex synchronisations or non-local withdrawals) are not easy to map onto (high-level) Petri nets. This inspired us to develop a new language by taking Petri nets as a starting point and adding mechanisms to allow for a more direct and intuitive support of the workflow patterns identified. This paper motivates the need for such a language, specifies the semantics of the language, and shows that soundness can be verified in a compositional way. Although YAWL is intended as a complete workflow language, the focus of this paper is limited to the control-flow perspective. {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
author = {{Van Der Aalst}, W. M.P. and {Ter Hofstede}, A. H.M.},
doi = {10.1016/j.is.2004.02.002},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/YAWL.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Petri nets,Workflow languages,Workflow management,Workflow patterns,YAWL},
number = {4},
pages = {245--275},
title = {{YAWL: Yet another workflow language}},
volume = {30},
year = {2005}
}
@article{Tarzey2019,
author = {Tarzey, Bob},
file = {:home/robert/Downloads/Getting to grips with low code.pdf:pdf},
pages = {17--21},
title = {{Getting to grips with low - code and no - code software development}},
year = {2019}
}
@article{Belloum2019,
abstract = {During the last several years, we have observed an exponential increase in the demand for Data Scientists in the job market. As a result, a number of trainings, courses, books, and university educational programs (both at undergraduate, graduate and postgraduate levels) have been labeled as “Big data” or “Data Science”; the fil-rouge of each of them is the aim at forming people with the right competencies and skills to satisfy the business sector needs. In this paper, we report on some of the exercises done in analyzing current Data Science education offer and matching with the needs of the job markets to propose a scalable matching service, ie, COmpetencies ClassificatiOn (E-CO-2), based on Data Science techniques. The E-CO-2 service can help to extract relevant information from Data Science–related documents (course descriptions, job Ads, blogs, or papers), which enable the comparison of the demand and offer in the field of Data Science Education and HR management, ultimately helping to establish the profession of Data Scientist.},
author = {Belloum, Adam S.Z. and Koulouzis, Spiros and Wiktorski, Tomasz and Manieri, Andrea},
doi = {10.1002/cpe.5200},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Belloum{\_}et{\_}al-2019-Concurrency{\_}and{\_}Computation{\_}{\_}Practice{\_}and{\_}Experience.pdf:pdf},
issn = {15320634},
journal = {Concurrency Computation },
keywords = {career development,data science,data science job market,education},
number = {17},
publisher = {John Wiley and Sons Ltd},
title = {{Bridging the demand and the offer in data science}},
volume = {31},
year = {2019}
}
@article{Preidel2017,
abstract = {Background: With the rising adoption of Building Information Modeling (BIM) in the AEC sector, computational models supersede traditional ways of information provision based on textual documents and two-dimensional drawings. The use of models enables the streamlining of workflows, and the included virtual construction increases the quality of the final product, the building. To create a comprehensive description of a planned building, information from different sources must be combined, specified and regularly updated by the project's stakeholders. The emerging models are highly structured, and instance files entail large amounts of data. However, in an unprocessed state, these models are of limited suitability for performing engineering tasks as the amount and structure does not match the domain-specific and purpose-oriented views. Methods: Selection and filtering data for the user's needs is a well-understood task in computer science, and various approaches are available. A promising approach is the usage of formal query languages. In this paper, selected common query languages are examined and assessed for processing building model information. Based on the analysis, we come to the conclusion that textual query languages are too complex to be employed by typical end users in the construction industry such as architects and engineers. Results: To overcome this issue, two Visual Programming Languages representing a new, more intuitive mechanism for data retrieval are introduced. The first one, QL4BIM, is designed for general filtering of IFC models, the second one, VCCL, has been developed for Code Compliance Checking. Both languages provide operators based on the Relational Algebra to allow handling of relations - a highly required feature of BIM QLs. Conclusions: The paper concludes with a discussion of the strengths and limitations of visual programming languages in the BIM context.},
author = {Preidel, Cornelius and Daum, Simon and Borrmann, Andr{\'{e}}},
doi = {10.1186/s40327-017-0055-0},
file = {:home/robert/Downloads/Data retrieval from building information.pdf:pdf},
issn = {22137459},
journal = {Visualization in Engineering},
keywords = {Building information modeling,Information retrieval,Relational algebra,Visual programming},
number = {1},
publisher = {Visualization in Engineering},
title = {{Data retrieval from building information models based on visual programming}},
volume = {5},
year = {2017}
}
@article{Tang2019,
abstract = {The problem of data visualization is to transform data into a visual context such that people can easily understand the significance of data. Nowadays, data visualization becomes especially important, because it is the de facto standard for modern business intelligence and successful data science. This tutorial will cover three specific topics: visualization languages define how the users can interact with various visualization systems; efficient data visualization processes the data and produces visualizations based on well-specified user queries; smart data visualization recommends data visualizations based on underspecified user queries. In this tutorial, we will go logically through these prior art, paying particular attentions on problems that may attract the interest from the database community.},
author = {Tang, Nan and Wu, Eugene and Li, Guoliang},
doi = {10.1145/3299869.3314029},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/towards democratizing data science.pdf:pdf},
isbn = {9781450356435},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {Data visualization,Relational data},
pages = {2025--2030},
title = {{Towards democratizing relational data visualization}},
year = {2019}
}
@article{Picado2018,
abstract = {Given a relational database and training examples for a target relation, relational learning algorithms learn a definition for the target relation in terms of the existing relations in the database. We propose a relational learning system called CastorX, which learns efficiently across multiple heterogeneous databases. The user specifies connections and relationships between different databases using a set of declarative constraints called matching dependencies (MDs). Each MD connects tuples across multiple databases that are related and can meaningfully join but the values of their join attributes may not be equal due to the different representations of these values in different databases. CastorX leverages these constraints during learning to find the information relevant to the training data and target definition across multiple databases. Since each tuple in a database may be connected to too many tuples in other databases according to an MD, the learning process will become very slow. Hence, CastorX uses sampling techniques to learn efficiently and output accurate definitions.},
author = {Picado, J. Jose and Termehchy, A. Arash and Pathak, S. Sudhanshu},
doi = {10.1145/3209889.3209897},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Learning effectively over realtional databases.pdf:pdf},
isbn = {9781450358286},
journal = {Proceedings of the 2nd Workshop on Data Management for End-To-End Machine Learning, DEEM 2018 - In conjunction with the 2018 ACM SIGMOD/PODS Conference},
pages = {1--4},
title = {{Learning efficiently over heterogeneous databases: Sampling and constraints to the rescue}},
year = {2018}
}
@article{Zolotas2018a,
abstract = {In the modern business world it is increasingly often that Enterprises opt to bring their business model online, in their effort to reach out to more end users and increase their customer base. While transitioning to the new model, enterprises consider securing their data of pivotal importance. In fact, many efforts have been introduced to automate this ‘webification' process; however, they all fall short in some aspect: a) they either generate only the security infrastructure, assigning implementation to the developers, b) they embed mainstream, less powerful authorisation schemes, or c) they disregard the merits of the dominating REST architecture and adopt less suitable approaches. In this paper we present RESTsec, a Low-Code platform that supports rapid security requirements modelling for Enterprise Services, abiding by the state of the art ABAC authorisation scheme. RESTsec enables the developer to seamlessly embed the desired access control policy and generate the service, the security infrastructure and the code. Evaluation shows that our approach is valid and can help developers deliver secure by design enterprise services in a rapid and automated manner.},
author = {Zolotas, Christoforos and Chatzidimitriou, Kyriakos C. and Symeonidis, Andreas L.},
doi = {10.1080/17517575.2018.1462403},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/RESTsec a low code platform for generating secure by design enterprise services.pdf:pdf},
issn = {17517583},
journal = {Enterprise Information Systems},
keywords = {ABAC,RESTful services,access control,data migration,model driven engineering},
number = {8-9},
pages = {1007--1033},
publisher = {Taylor {\&} Francis},
title = {{RESTsec: a low-code platform for generating secure by design enterprise services}},
url = {https://doi.org/10.1080/17517575.2018.1462403},
volume = {12},
year = {2018}
}
@article{Wan2018,
abstract = {To improve the scalability and elasticity of application deployment and operation in cloud computing environments, new architectures and techniques are developed and studied, e.g., microservice architecture, and Docker container. Especially, Docker container enables the sharing on operation system and supporting libraries, which is more lightweight, prompt and scalable than Hypervisor based virtualization. These features make it ideally suit for applications deployed in microservice architecture. However, existing models and schemes, which are mostly designed for Hypervisor based virtualization techniques, fall short to be efficiently used for Docker container based application deployment. To take the benefits of microservice architecture and Docker containers, we explore the optimization of application deployment in cloud data centers using microservice and Docker containers. Our goal is to minimize the application deployment cost as well as the operation cost while preserving service delay requirements for applications. In this paper, we first formulate the application deployment problem by examining the features of Docker, the requirements of microservice-based applications, and available resources in cloud data centers. We further propose a communication efficient framework and a suboptimal algorithm to determine the container placement and task assignment. The proposed algorithm works in a distributed and incremental manner, which makes it scalable to massive physical resources and diverse applications under the framework. We validate the efficiency of our solution through comparisons with three existing strategies in Docker Swarm using real traces from Google Cluster. The evaluation results show that the proposed framework and algorithm provide more flexibility and save more cost than existing strategies.},
author = {Wan, Xili and Guan, Xinjie and Wang, Tianjing and Bai, Guangwei and Choi, Baek Yong},
doi = {10.1016/j.jnca.2018.07.003},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/To print/Application deployment using docker and microservices.pdf:pdf},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Application deployment,Docker container,Microservice architecture},
number = {December 2017},
pages = {97--109},
publisher = {Elsevier Ltd},
title = {{Application deployment using Microservice and Docker containers: Framework and optimization}},
url = {https://doi.org/10.1016/j.jnca.2018.07.003},
volume = {119},
year = {2018}
}
@article{Seek2017,
author = {Seek, Companies and Improve, To},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/To print/Containers{\_}Real{\_}Adoption{\_}2017{\_}Dell{\_}EMC{\_}Forrester{\_}Paper.pdf:pdf},
number = {March},
title = {{Containers : Real Adoption And Use Cases In 2017 1 Executive Summary 2 Containers Are Breaking Out Of The Test / Dev Environment 5 Organizations Are Modernizing Applications — Or Are They ? 7 Containerization Will Grow As Organizations Seek To Improve Eff}},
year = {2017}
}
@article{Richardson2016,
author = {Richardson, Clay and Rymer, John R},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/0eb07c59-b01c-4399-9022-dfc297487060{\_}Forrester Vendor Landscape The Fractured, Fertile Terrain.pdf:pdf},
title = {{Vendor Landscape: The Fractured, Fertile Terrain Of Low-Code Application Platforms}},
url = {Forrester Research database},
year = {2016}
}
@article{Dimyadi2016,
abstract = {In the AEC industry, there is a large number of standards and codes which ensure the structural stability, reliability, usability of the building under design. Accordingly, checking the conformity of the building design with these requirements is a crucial process. Nowadays this checking is performed to a large extent manually based on two-dimensional technical drawings and textual documents. Due to the low level of automation, the conventional checking procedure is laborious, cumbersome and error-prone. As Building Information Modeling (BIM) becomes more and more mature, a suitable digital information basis also becomes available to enable automating the process. The commercial solutions for code compliance checking available so far mainly follow a black-box approach where the rules that make up a certain regulation are implemented in a hard-wired fashion rendering their implementation in-transparent and non-extendable. A number of researchers have tackled this problem and have proposed various ways that allow the user to define rules, either in a standard programming language or in a dedicated language. However, AEC domain experts usually do not have the required programming skills to use these languages appropriately. To overcome this issue, we introduce the Visual Code Checking Language (VCCL), which uses a graphical notation in order to represent the rules of a code in a machine- and human-readable language. The paper presents the features and functionalities of the VCCL in detail and shows its application in a number of case studies. COPYRIGHT:},
author = {Dimyadi, J. and Solihin, W. and Preidel, Cornelius and Borrmann, Andr{\'{e}}},
file = {:home/robert/Downloads/TOWARDS CODE COMPLIANCE CHECKING ON THE BASIS OF A.pdf:pdf},
issn = {14006529},
journal = {Journal of Information Technology in Construction},
keywords = {Building Information Modeling,Code compliance checking,Visual programming language},
number = {July},
pages = {402--421},
title = {{Towards code compliance checking on the basis of a visual programming language}},
volume = {21},
year = {2016}
}
@article{Science2011,
author = {Science, Democratizing},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Globus online.pdf:pdf},
pages = {1--31},
publisher = {IEEE},
title = {{Globus Online}},
url = {papers3://publication/uuid/5162918B-497A-4A25-8EDF-B50C9AFE2E47},
year = {2011}
}
@article{Cuvic2018,
abstract = {Understanding program execution is mandatory if a programmer is to write code, regardless of programming experience. Therefore, it is important for novice programmers to construct correct mental models of the execution of the notional machine. To this end, many program visualizations have been developed over the years. However, novice programmers often focus on learning the syntax of a programming language rather than programming itself. Dataflow visual programming languages (DFVPL) allow us to build programs by connecting blocks with arcs. In this paper we present our own DFVPL that exhibits a high level of responsiveness to user inputs and allows the user to control the execution of the program.},
author = {{\v{C}}uvi{\'{c}}, Marin Agli{\'{c}}},
file = {:home/robert/Downloads/11-52-1-PB.pdf:pdf},
keywords = {dataflow,dataflow visual programming,learning programming,program visualizations},
number = {1},
title = {{Introducing a Dataflow visual programming language for understanding program execution}},
volume = {2},
year = {2018}
}
@article{Dadzie2015,
abstract = {The era of Big Data brings with it the need to develop new skills for managing this heterogenous, complex, large scale knowledge source, to extract its content for effective task completion and informed decision-making. Defining these skills and mapping them to demand is a first step in meeting this challenge. We discuss the outcomes of visual exploratory analysis of demand for Data Scientists in the EU, examining skill distribution across key industrial sectors and geolocation for two snapshots in time. Our aim is to translate the picture of skill capacity into a formal specification of user, task and data requirements for demand analysis. The knowledge thus obtained will be fed into the development of context-sensitive learning resources to fill the skill gaps recognised.},
author = {Dadzie, Aba Sah and Domingue, John},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Visual exploration of formal requirements for data science demand analysis.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Big Data,Data-driven decision-making,Demand analysis,Ontology-guided design,RTD,Visual analytics,Visual exploration},
pages = {1--12},
title = {{Visual exploration of formal requirements for data science demand analysis}},
volume = {1456},
year = {2015}
}
@article{Stahl2015,
author = {Stahl, Christian},
doi = {10.1007/11538394},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Yet{\_}Another{\_}Event-Driven{\_}Process{\_}Chain.pdf:pdf},
keywords = {analysis,bpel,business process modeling and,formal models in busi-,ness process management,petri nets,process verification and validation},
number = {September},
title = {{Transforming BPEL to Petri nets Transforming BPEL to Petri Nets}},
year = {2015}
}
@article{Date2018,
author = {Date, Publication},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Democratizing data science.pdf:pdf},
title = {{UC Santa Barbara UC Santa Barbara Electronic Theses and Dissertations Towards Democratizing Data Science with Natural Language Interfaces}},
year = {2018}
}
@book{Knudsen2019,
author = {Knudsen, M. S. and Kaivo-oja, J. and Lauraeus, T.},
doi = {10.1007/978-3-030-21451-7},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/2019{\_}Book{\_}KnowledgeManagementInOrganizat.pdf:pdf},
isbn = {9783030214517},
keywords = {Cont,Ontology entities distributed representations,concept embeddings,continuous vector representations,feature representation,neural networks,ontology entities distributed representations},
pages = {3--13},
title = {{Efficient Estimation of Ontology Entities [Estimaci{\'{o}}n eficiente de entidades de ontolog{\'{i}}a]}},
url = {http://dx.doi.org/10.1007/978-3-030-21451-7{\_}5},
volume = {1},
year = {2019}
}
@article{Shang2019a,
abstract = {Statistical knowledge and domain expertise are key to extract actionable insights out of data, yet such skills rarely coexist together. In Machine Learning, high-quality results are only attainable via mindful data preprocessing, hyperparameter tuning and model selection. Domain experts are often overwhelmed by such complexity, de-facto inhibiting a wider adoption of ML techniques in other elds. Existing libraries that claim to solve this problem, still require well-trained practitioners. Those frameworks involve heavy data preparation steps and are often too slow for interactive feedback from the user, severely limiting the scope of such systems. In this paper we present Alpine Meadow, arst Interactive Automated Machine Learning tool. What makes our system unique is not only the focus on interactivity, but also the combined systemic and algorithmic design approach; on one hand we leverage ideas from query optimization, on the other we devise novel selection and pruning strategies combining cost-based Multi-Armed Bandits and Bayesian Optimization. We evaluate our system on over 300 datasets and compare against other AutoML tools, including the current NIPS winner, as well as expert solutions. Not only is Alpine Meadow able to signicantly outperform the other AutoML systems while - in contrast to the other systems - providing interactive latencies, but also outperforms in 80{\%} of the cases expert solutions over data sets we have never seen before.},
author = {Shang, Zeyuan and Zgraggen, Emanuel and Buratti, Benedetto and Kossmann, Ferdinand and Eichmann, Philipp and Chung, Yeounoh and Binnig, Carsten and Upfal, Eli and Kraska, Tim},
doi = {10.1145/3299869.3319863},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Democratizing Data Science through Interactive.pdf:pdf},
isbn = {9781450356435},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
pages = {1171--1188},
title = {{Democratizing data science through interactive curation of ML pipelines}},
year = {2019}
}
@article{Binnig2019,
abstract = {Enabling interactive visualization over new datasets at “human speed” is key to democratizing data science and maximizing human productivity. In this work, we first argue why existing analytics infrastructures do not support interactive data exploration and outline the challenges and opportunities of building a system specifically designed for interactive data exploration. Furthermore, we present the results of building IDEA, a new type of system for interactive data exploration that is specifically designed to integrate seamlessly with existing data management landscapes and allow users to explore their data instantly without expensive data preparation costs. Finally, we discuss other important considerations for interactive data exploration systems including benchmarking, natural language interfaces, as well as interactive machine learning.},
author = {Binnig, Carsten and Basık, Fuat and Buratti, Benedetto and Cetintemel, Ugur and Chung, Yeounoh and Crotty, Andrew and Cousins, Cyrus and Ebert, Dylan and Eichmann, Philipp and Galakatos, Alex and H{\"{a}}ttasch, Benjamin and Ilkhechi, Amir and Kraska, Tim and Shang, Zeyuan and Tromba, Isabella and Usta, Arif and Utama, Prasetya and Upfal, Eli and Wang, Linnan and Weir, Nathaniel and Zeleznik, Robert and Zgraggen, Emanuel},
doi = {10.1007/978-3-030-24124-7_11},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Towards{\_}interactive{\_}data{\_}exploration.pdf:pdf},
isbn = {9783030241230},
issn = {18651356},
journal = {Lecture Notes in Business Information Processing},
pages = {177--190},
title = {{Towards Interactive Data Exploration}},
volume = {337},
year = {2019}
}
@article{Crotty2016,
abstract = {Enabling interactive visualization over new datasets at "human speed" is key to democratizing data science and maximizing human productivity. In this work, we first argue why existing analytics infrastructures do not support interactive data exploration and then outline the challenges and opportunities of building a system specifically designed for interactive data exploration. Finally, we present an Interactive Data Exploration Accelerator (IDEA), a new type of system for interactive data exploration that is specifically designed to integrate with existing data management landscapes and allow users to explore their data instantly without expensive data preparation costs.},
author = {Crotty, Andrew and Galakatos, Alex and Zgraggen, Emanuel and Binnig, Carsten and Kraska, Tim},
doi = {10.1145/2939502.2939513},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Interactive data visalisation .pdf:pdf},
isbn = {9781450342070},
journal = {HILDA 2016 - Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
title = {{The case for Interactive Data Exploration Accelerators (IDEAs)}},
year = {2016}
}
@article{Tonelli2019,
abstract = {Blockchain technologies and smart contracts are becoming mainstream research fields in computer science and researchers are continuously investigating new frontiers for new applications. Likewise, microservices are getting more and more popular in the latest years thanks to their properties, that allow teams to slice existing information systems into small and independent services that can be developed independently by different teams.A symmetric paradigm applies to smart contracts as well, which represent well defined, usually isolated, executable programs, typically implementing simple and autonomous tasks with a well defined purpose, which can be assumed as services provided by the Contract. In this work we analyze a concrete case study where the microservices architecture environment is replicated and implemented through an equivalent set of smart contracts, showing for the first time the feasibility of implementing a microservices-based system with smart contracts and how the two innovative paradigms match together.Results show that it is possible to implement a simple microservices-based system with smart contracts maintaining the same set of functionalities and results. The result could be highly beneficial in contexts such as smart voting, where not only the data integrity is fundamental but also the source code executed must be trustable.},
author = {Tonelli, Roberto and Lunesu, Maria Ilaria and Pinna, Andrea and Taibi, Davide and Marchesi, Michele},
doi = {10.1109/IWBOSE.2019.8666520},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Blockchain microservices.pdf:pdf},
isbn = {9781728118079},
journal = {IWBOSE 2019 - 2019 IEEE 2nd International Workshop on Blockchain Oriented Software Engineering},
keywords = {Blockchain,Cloud Native,Microservice,Serverless,Smart contract},
pages = {22--31},
publisher = {IEEE},
title = {{Implementing a Microservices System with Blockchain Smart Contracts}},
year = {2019}
}
@book{TerHofstede2010,
abstract = {Topics covered include: .The fundamentals of business process modeling, including workflow patterns; .An in-depth treatment of process flexibility, including approaches to dealing with on-the-fly changes, unexpected exceptions, and constraint-based processes; .Technological aspects of a modern BPM environment, including its architecture, process design environment, process engine, resource handler and other support services; .A comparative insight into current approaches to business process modeling and execution such as BPMN, EPCs, BPEL, jBPM, OpenWFE, and Enhydra Shark; .Process mining, verification, integration and configuration; and .Case studies in health care and screen business. This book provides a comprehensive treatment of the field of Business Process Management (BPM) with a focus on Business Process Automation. It achieves this by covering a wide range of topics, both introductory and advanced, illustrated through and grounded in the YAWL (Yet Another Workflow Language) language and corresponding open-source support environment. In doing so it provides the reader with a deep, timeless, and vendor-independent understanding of the essential ingredients of business process automation. The BPM field is in a continual state of flux and is subject to both the ongoing proposal of new standards and the introduction of new tools and technology. Its fundamentals however are relatively stable and this book aims to equip the reader with both a thorough understanding of them and the ability to apply them to better understand, assess and utilize new developments in the BPM field. As a consequence of its topic-based format and the inclusion of a broad range of exercises, the book is eminently suitable for use in tertiary education, both at the undergraduate and the postgraduate level, for students of computer science and information systems. BPM researchers and practitioners will also find it a valuable resource. The book serves as a unique reference to a varied and comprehensive collection of topics that are relevant to the business process life-cycle. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
author = {{Ter Hofstede}, Arthur H.M. and {Van Der Aalst}, Wil M.P. and Adams, Michael and Russell, Nick},
booktitle = {Modern Business Process Automation: YAWL and its Support Environment},
doi = {10.1007/978-3-642-03121-2},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Modern Business Process automation - YAWL.pdf:pdf},
isbn = {9783642031205},
pages = {1--676},
title = {{Modern business process automation: YAWL and its support environment}},
year = {2010}
}
@techreport{Siddiqui2150,
abstract = {Data visualization is by far the most commonly used mechanism to explore and extract insights from datasets, especially by novice data scientists. And yet, current visual analytics tools are rather limited in their ability to operate on collections of visualizations-by composing , filtering, comparing, and sorting them-to find those that depict desired trends or patterns. The process of visual data exploration remains a tedious process of trial-and-error. We propose zenvisage, a visual analytics platform for effortlessly finding desired visual patterns from large datasets. We introduce zenvisage's general purpose visual exploration language, ZQL ("zee-quel") for specifying the desired visual patterns, drawing from use-cases in a variety of domains, including biology, mechanical engineering, climate science, and commerce. We formalize the expressiveness of ZQL via a visual exploration algebra-an algebra on collections of visualizations-and demonstrate that ZQL is as expressive as that algebra. zenvisage exposes an interactive front-end that supports the issuing of ZQL queries, and also supports interactions that are "short-cuts" to certain commonly used ZQL queries. To execute these queries, zenvisage uses a novel ZQL graph-based query opti-mizer that leverages a suite of optimizations tailored to the goal of processing collections of visualizations in certain pre-defined ways. Lastly, a user survey and study demonstrates that data scientists are able to effectively use zenvisage to eliminate error-prone and tedious exploration and directly identify desired visualizations.},
author = {Siddiqui, Tarique and Kim, Albert and Lee, John and Karahalios, Karrie and Parameswaran, Aditya},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Effortless data exploration with zenvisage$\backslash$: an expressive and interactive visual analytics system
.pdf:pdf},
title = {{Effortless Data Exploration with zenvisage: An Expressive and Interactive Visual Analytics System}},
year = {2150}
}
@article{Kim2016,
abstract = {As data has become critical to our everyday lives, a growing concern with the skills gap required to exploit the data surfeit has arisen; library and information science practitioners and educators have recognized this concern. This paper is intended to identify current trends in library and information science education in response to the rising demand for data professionals. To provide a detailed map of the content of the current curriculum, academic programs and courses that support a data-driven workforce offered by library schools in North America were reviewed. The results of this analysis indicates that various topics are being offered to address skills gaps for data professionals, but there are still insufficient opportunities for students to develop the depth and breadth of knowledge and skills needed to be highly capable data professionals. It is suggested that cross-disciplinary and/or cross-institutional collaboration may be an efficient way to enhance and develop educational and training opportunities for data professionals.},
author = {Kim, Jeonghyun},
doi = {10.12783/issn.2328-2967/57/2/8},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Meeting the demand for data science professionals.pdf:pdf},
issn = {0748-5786},
journal = {Journal of Education for Library and Information Science Online},
keywords = {academic,big data,curriculum analysis,data professionals,libraries,lis education,research skills},
number = {2},
pages = {161--173},
title = {{Who is Teaching Data: Meeting the Demand for Data Professionals}},
volume = {57},
year = {2016}
}
@article{Waszkowski2019,
abstract = {The low-code platform enables quick generation and delivery of business applications with minimum effort to write in a coding language and requires the least possible effort for the installation and configuration of environments, and training and implementation. With a rapidly growing number of companies, the use of low-code solutions can be a significant step forward in creating essential business applications. This paper describes the use of the Aurea BPM low-code platform for automating business processes in manufacturing.},
author = {Waszkowski, Robert},
doi = {10.1016/j.ifacol.2019.10.060},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Low-code platform for automating business processes in manufacturing.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Aurea BPM,Low-code platform,business processes,manufacturing,process automation},
number = {10},
pages = {376--381},
publisher = {Elsevier Ltd},
title = {{Low-code platform for automating business processes in manufacturing}},
url = {https://doi.org/10.1016/j.ifacol.2019.10.060},
volume = {52},
year = {2019}
}
@article{Cao2017,
abstract = {WHILE DATA SCIENCE has emerged as an ambitious new scientific field, related debates and discussions have sought to address why science in general needs data science and what even makes data science a science. However, few such discussions concern the intrinsic complexities and intelligence in data science.},
author = {Cao, Longbing},
doi = {10.1145/3015456},
file = {:home/robert/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao - 2017 - Data science Challenges and directions.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {8},
pages = {59--68},
title = {{Data science: Challenges and directions}},
volume = {60},
year = {2017}
}
@article{Taibi2019,
abstract = {Decomposition is one of the most complex tasks during the migration from monolithic systems to microservices, generally performed manually, based on the experience of the software architects. In this work, we propose a 6-step framework to reduce the subjectivity of the decomposition process. The framework provides software architects with a set of decomposition options, together with a set of measures to evaluate and compare their quality. The decomposition options are identified based on the independent execution traces of the system by means of the application of a process-mining tool to the log traces collected at runtime. We validated the process, in an industrial project, by comparing the proposed decomposition options with the one proposed by the software architect that manually analyzed the system. The application of our framework allowed the company to identify issues in their software that the architect did not spot manually, and to discover more suitable decomposition options that the architect did not consider. The framework could be very useful also in other companies to improve the quality of the decomposition of any monolithic system, identifying different decomposition strategies and reducing the subjectivity of the decomposition process. Moreover, researchers could extend our approach increasing the support and further automating the decomposition support.},
author = {Taibi, Davide and Syst{\"{a}}, Kari},
doi = {10.5220/0007755901530164},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/CLOSER{\_}2019{\_}{\_}{\_}From{\_}Monolithic{\_}Systems{\_}to{\_}Microservices{\_}{\_}A{\_}Decomposition{\_}Framework{\_}based{\_}on{\_}Process{\_}Mining-9.pdf:pdf},
isbn = {9789897583650},
journal = {CLOSER 2019 - Proceedings of the 9th International Conference on Cloud Computing and Services Science},
keywords = {Cloud-native,Microservice decomposition,Microservice migration,Microservice slicing,Microservices},
number = {May},
pages = {153--164},
title = {{From monolithic systems to microservices: A decomposition framework based on process mining}},
year = {2019}
}
@article{Scolati2019,
abstract = {The constant increase of the amount of data generated by Internet of Things (IoT) devices creates challenges for the supporting cloud infrastructure, which is often used to process and store the data. This work focuses on an alternative approach, based on the edge cloud computing model, i.e., processing and filtering data before transferring it to a backing cloud infrastructure. We describe the implementation of a low-power and low-cost cluster of single board computers (SBC) for this context, applying models and technologies from the Big Data domain with the aim of reducing the amount of data which has to be transferred elsewhere. To implement the system, a cluster of Raspberry Pis was built, relying on Docker to containerize and deploy an Apache Hadoop and Apache Spark cluster, on which a test application is then executed. A monitoring stack based on Prometheus, a popular monitoring and alerting tool in the cloud-native industry, is used to gather system metrics and analyze the performance of the setup. We evaluate the complexity of the system, showing that by means of containerization increased fault tolerance and ease of maintenance can be achieved, which makes the proposed solution suitable for an industrial environment. Furthermore, an analysis of the overall performance, which takes into account the resource usage of the proposed solution with regards to the constraints imposed by the devices, is presented in order to discuss the capabilities and limitations of proposed architecture.},
author = {Scolati, Remo and Fronza, Ilenia and {El Ioini}, Nabil and Samir, Areeg and Pahl, Claus},
doi = {10.5220/0007695000680080},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Containerized big data streaming architecture for edge cloud computing.pdf:pdf},
isbn = {9789897583650},
journal = {CLOSER 2019 - Proceedings of the 9th International Conference on Cloud Computing and Services Science},
keywords = {Big data,Cluster architecture,Container technology,Data streaming,Docker swarm,Edge cloud,Performance engineering,Raspberry Pi},
number = {May},
pages = {68--80},
title = {{A containerized big data streaming architecture for edge cloud computing on clustered single-board devices}},
year = {2019}
}
@article{Berman2018,
author = {Berman, Francine and Rutenbar, Rob and Hailpern, Brent and Christensen, Henrik and Davidson, Susan and Estrin, Deborah and Franklin, Michael and Martonosi, Margaret and Raghavan, Padma and Stodden, Victoria and Szalay, Alexander S.},
doi = {10.1145/3188721},
file = {:home/robert/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berman et al. - 2018 - Realizing the potential of data science.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {4},
pages = {67--72},
title = {{Realizing the potential of data science}},
volume = {61},
year = {2018}
}
@article{Clark2019,
author = {Clark, Lindsay},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/LOW-CODE MATURITY BOOSTS EFFICIENCY AND HELPS USER ACCEPTANCE..pdf:pdf},
journal = {Comptuerweekly.com},
number = {November},
pages = {18--23},
title = {{Low-code maturity boosts efficiency and helps user acceptance}},
year = {2019}
}
@article{Joao2019,
abstract = {In the last few years, it has been pointed out that teaching programming is a strong strategy to develop pupils' competences in computational thinking (CT). In the Portuguese context, the curriculum changes in 2018 made programming and CT compulsory for every pupil in primary and secondary education. Nowadays, there is an information and communication technology (ICT) subject, taught by a computer science teacher in each school grade. In Portugal, to become a computer science teacher in primary and secondary education, it is compulsory to have a master's degree in computer science education. This article reports on a pedagogical activity developed with student-teachers of a Master in Teaching Informatics at the University of Lisbon. Within the activities of the master's program, we developed a cross-analysis of the core characteristics of 26 block-based and visual programming applications (apps) used to teach computational thinking and programming in school classes. In order to organize the analysis, a framework with several dimensions was developed and used by student-teachers to register the characteristics of each app. The product of this work is a comparative matrix mapping the core characteristics of each of the 26 apps that student-teachers used to select the most appropriate one for teaching programming and computational thinking according to each grade, age group and other characteristics.},
author = {Jo{\~{a}}o, Piedade and Nuno, Dorotea and F{\'{a}}bio, Sampaio Ferrentini and Ana, Pedro},
doi = {10.3390/educsci9030181},
file = {:home/robert/Downloads/A Cross-analysis of Block-based and Visual Programming App.pdf:pdf},
issn = {22277102},
journal = {Education Sciences},
keywords = {Computational thinking,Computer science education,Cross-analysis,Programming,Visual programming applications},
number = {3},
title = {{A cross-analysis of block-based and visual programming apps with computer science student-teachers}},
volume = {9},
year = {2019}
}
@article{Tisi2019,
abstract = {Low-Code Development Platforms (LCDPs) are software development platforms on the Cloud, provided through a Platform-as-a-Service model, which allow users to build completely operational applications by interacting through dynamic graphical user interfaces, visual diagrams and declarative languages. Lowcomote will train a generation of experts that will upgrade the current trend of LCDP to a new paradigm, Low-Code Engineering Platform. This will be achieved by injecting in LCDPs the theoretical and technical framework defined by recent research in Model Driven Engineering, augmented with Cloud Computing and Machine Learning techniques.},
author = {Tisi, Massimo and Mottu, Jean Marie and Kolovos, Dimitrios S. and de Lara, Juan and Guerra, Esther and {Di Ruscio}, Davide and Pierantonio, Alfonso and Wimmer, Manuel},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Training the Next Generation of Experts.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Lowcomote: Training the next generation of experts in scalable low-code engineering platforms}},
volume = {2405},
year = {2019}
}
@article{Zolotas2018,
abstract = {In the modern business world it is increasingly often that Enterprises opt to bring their business model online, in their effort to reach out to more end users and increase their customer base. While transitioning to the new model, enterprises consider securing their data of pivotal importance. In fact, many efforts have been introduced to automate this ‘webification' process; however, they all fall short in some aspect: a) they either generate only the security infrastructure, assigning implementation to the developers, b) they embed mainstream, less powerful authorisation schemes, or c) they disregard the merits of the dominating REST architecture and adopt less suitable approaches. In this paper we present RESTsec, a Low-Code platform that supports rapid security requirements modelling for Enterprise Services, abiding by the state of the art ABAC authorisation scheme. RESTsec enables the developer to seamlessly embed the desired access control policy and generate the service, the security infrastructure and the code. Evaluation shows that our approach is valid and can help developers deliver secure by design enterprise services in a rapid and automated manner.},
author = {Zolotas, Christoforos and Chatzidimitriou, Kyriakos C. and Symeonidis, Andreas L.},
doi = {10.1080/17517575.2018.1462403},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/RESTsec$\backslash$: a low-code platform for generating.pdf:pdf},
issn = {17517583},
journal = {Enterprise Information Systems},
keywords = {ABAC,RESTful services,access control,data migration,model driven engineering},
number = {8-9},
pages = {1007--1033},
publisher = {Taylor {\&} Francis},
title = {{RESTsec: a low-code platform for generating secure by design enterprise services}},
url = {https://doi.org/10.1080/17517575.2018.1462403},
volume = {12},
year = {2018}
}
@article{Quiroz-Fabian2019,
abstract = {Parallel programming continues to be a challenging task despite the many advances in parallel architectures and their wide availability in the cloud. The need both to partition the workload among various processing elements and to specify communication between them to share code and data, and to coordinate their tasks, requires from the developer a deep understanding of the problem, the parallel architecture and the programming language used in order to develop efficient parallel applications. This problem can be reduced significantly through the use of visual programming languages to hide most aspects related to the specification of communication and processes management. This paper presents VPPE, a novel Visual Parallel Programming Environment that allows developers to program parallel applications through organising workflows of interconnected icons. VPPE is a cloud environment that supports icons for specifying: I/O operations, workflow organisation, communication, and processing. Processing computing patterns supported so far include Single Program Multiple Data, Multiple Program Multiple Data, Pipeline, and Master–Slave. The paper highlights the design of VPPE based on a context-free graph grammar, its current implementation based on Java-MPI, its use in developing various parallel applications, and its evaluation compared to Java-MPI text-based programming.},
author = {Quiroz-Fabi{\'{a}}n, Jos{\'{e}} L. and Rom{\'{a}}n-Alonso, Graciela and Castro-Garc{\'{i}}a, Miguel A. and Buenabad-Ch{\'{a}}vez, Jorge and Boukerche, Azzedine and Aguilar-Cornejo, Manuel},
doi = {10.1007/s10766-019-00639-w},
file = {:home/robert/Downloads/VPPE$\backslash$: A Novel Visual Parallel Programming Environment.pdf:pdf},
issn = {15737640},
journal = {International Journal of Parallel Programming},
keywords = {Cloud computing,Graph grammar,Hyperedge replacement grammar,Parallel patterns,Workflow},
number = {5-6},
pages = {1117--1151},
title = {{VPPE: A Novel Visual Parallel Programming Environment}},
volume = {47},
year = {2019}
}
@book{Bernhaupt2017,
author = {Bernhaupt, Regina and Dalvi, Girish and Neill, Jacki O and Winckler, Marco and Hutchison, David},
doi = {10.1007/978-3-319-67687-6},
file = {:home/robert/Downloads/2017{\_}Book{\_}Human-ComputerInteractionINTER.pdf:pdf},
isbn = {9783319676869},
issn = {03029743},
keywords = {parkinson,pd,s disease},
pages = {395--403},
title = {{INTERACT 2017 Part III}},
volume = {1},
year = {2017}
}
@article{McCormick2007,
abstract = {This paper develops a framework for understanding social movements that address issues related to science, technology and expert knowledge. 'Democratizing science movements' contest, reframe, and engage in the production of official scientific research in order to achieve their goals. They contest the seeming objectivity and neutrality of science and seek to legitimate lay perspectives. In order to empirically explore why such movements arise and how they work, I discuss two cases: the anti-dam movement in Brazil and the environmental breast cancer movement in the USA. While there are obvious internal and contextual differences between these two movements, they both exemplify similar characteristics of democratizing science movements. In this sense, these cases are representative of a broader, transnational phenomenon. Qualitative data in the form of interviews, ethnographic observations and document collection were used to study these cases. {\textcopyright} SSS and SAGE Publications.},
author = {McCormick, Sabrina},
doi = {10.1177/0306312707076598},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Democratizing science movements.pdf:pdf},
issn = {03063127},
journal = {Social Studies of Science},
keywords = {Brazil,Breast cancer,Environment,Science,Social movements},
number = {4},
pages = {609--623},
title = {{Democratizing science movements: A new framework for mobilization and contestation}},
volume = {37},
year = {2007}
}
@book{Goos1980,
author = {Goos, G. and Hartmanis, J.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-662-19161-3},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Design and implementation of YAWL.pdf:pdf},
isbn = {3540221514},
issn = {16113349},
pages = {i},
title = {{Lecture notes in computer science}},
volume = {106},
year = {1980}
}
@article{Kang2016,
abstract = {Emerging container technologies, such as Docker, offer unprecedented agility in developing and running applications in cloud environment especially when combined with a microservice-style architecture. However, it is often difficult to use containers to manage the cloud infrastructure, without sacrificing many benefits container offers. This paper identifies the key challenges that impede realizing the full promise of containerizing infrastructure services. Using OpenStack as a case study, we explore solutions to these challenges. Specifically, we redesign OpenStack deployment architecture to enable dynamic service registration and discovery, explore different ways to manage service state in containers, and enable containers to access the host kernel and devices. We quantify the efficiency of the container-based microservice-style DevOps compared to the VM-based approach, and study the scalability of the stateless and stateful containerized components. We also discuss limitations in our current design, and highlight open research problems that, if solved, can lead to wider adoption of containers in cloud infrastructure management.},
author = {Kang, Hui and Le, Michael and Tao, Shu},
doi = {10.1109/IC2E.2016.26},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/To print/Container and Microservice Driven Design for Cloud Infrastructure DevOps:},
isbn = {9781509019618},
journal = {Proceedings - 2016 IEEE International Conference on Cloud Engineering, IC2E 2016: Co-located with the 1st IEEE International Conference on Internet-of-Things Design and Implementation, IoTDI 2016},
keywords = {Cloud computing,DevOps,OpenStack,container},
pages = {202--211},
publisher = {IEEE},
title = {{Container and microservice driven design for cloud infrastructure DevOps}},
year = {2016}
}
@article{Cities2012,
author = {Cities, Smart},
file = {:home/robert/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cities - 2012 - DG CONNECT A European strategy on the data value chain.pdf:pdf},
number = {2011},
pages = {1--28},
title = {{DG CONNECT A European strategy on the data value chain}},
year = {2012}
}
@book{Stopka2018,
abstract = {Driven by megatrends such as globalization, urbanization, climate change and technological progress, the mobility sector is undergoing a strong process of change which is characterized in particular by the intermodal cross-linking of various public and private mobility services. The aim is to make transport as a whole more environmentally friendly. To meet this challenge “Mobility as a Service” (MaaS) concepts are introduced in the market which offer individualized one-stop access to several bundled travel services based on customer's needs. The supply of so-called mobility packages requires very close cooperation between the various players on the transport market who use electronic platforms for this purpose. First of all, the paper gives an overview about the research activities and the implementation status of MaaS concepts in different countries. In the following chapters, the general approach and methods for the development of mobility packages are discussed and first results of related research projects in Germany are presented.},
author = {Stopka, Ulrike and Pessier, Ren{\'{e}} and G{\"{u}}nther, Christian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-91244-8_34},
file = {:home/robert/Downloads/2018{\_}Book{\_}Human-ComputerInteractionInter.pdf:pdf},
isbn = {9783319912431},
issn = {16113349},
keywords = {Conjoint analysis,Intermodal transport,Mobility as a Service,Mobility packages,Mobility platform,Urban mobility,User requirements,Van Westendorp analysis},
pages = {419--439},
title = {{Mobility as a service (MaaS) based on intermodal electronic platforms in public transport}},
volume = {10902 LNCS},
year = {2018}
}
@book{Kulkarni2018,
author = {Kulkarni, Ketki and B, Aris Pagourtzis and Potika, Katerina and Potikas, Petros},
doi = {10.1007/978-3-030-19759-9},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/2019{\_}Book{\_}AlgorithmicAspectsOfCloudCompu.pdf:pdf},
isbn = {9783030197599},
keywords = {Community detection,Hierarchi,Neighborhood overlap,community detection,edge betweenness,hierarchical clustering,modularity,neighborhood overlap,social networks,spanning trees},
pages = {13--24},
title = {{Algorithmic Aspects of Cloud Computing}},
url = {http://dx.doi.org/10.1007/978-3-030-19759-9{\_}2},
volume = {2},
year = {2018}
}
@article{Perez2018,
abstract = {New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs.},
author = {P{\'{e}}rez, Alfonso and Molt{\'{o}}, Germ{\'{a}}n and Caballer, Miguel and Calatrava, Amanda},
doi = {10.1016/j.future.2018.01.022},
file = {:home/robert/Documents/Documents/Bath/Dissertation/References/Serverless computing for container-based archiectures.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {AWS lambda,Cloud computing,Docker,Elasticity,Serverless},
pages = {50--59},
publisher = {Elsevier B.V.},
title = {{Serverless computing for container-based architectures}},
url = {https://doi.org/10.1016/j.future.2018.01.022},
volume = {83},
year = {2018}
}
@misc{McKinsey,
year= {2011},
title = {{Big data: The next frontier for innovation, competition, and productivity | McKinsey}},
url = {https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/big-data-the-next-frontier-for-innovation},
urldate = {2020-04-17}
}

@book{CookJoshua2017DfDS,
abstract = {Learn Docker "infrastructure as code" technology to define a system for performing standard but non-trivial data tasks on medium- to large-scale data sets, using Jupyter as the master controller. It is not uncommon for a real-world data set to fail to be easily managed. The set may not fit well into access memory or may require prohibitively long processing. These are significant challenges to skilled software engineers and they can render the standard Jupyter system unusable.  As a solution to this problem, Docker for Data Science proposes using Docker. You will learn how to use existing pre-compiled public images created by the major open-source technologies—Python, Jupyter, Postgres—as well as using the Dockerfile to extend these images to suit your specific purposes. The Docker-Compose technology is examined and you will learn how it can be used to build a linked system with Python churning data behind the scenes and Jupyter managing these background tasks. Best practices in using existing images are explored as well as developing your own images to deploy state-of-the-art machine learning and optimization algorithms.   What  You'll Learn: Master interactive development using the Jupyter platform Run and build Docker containers from scratch and from publicly available open-source images Write infrastructure as code using the docker-compose tool and its docker-compose.yml file type Deploy a multi-service data science application across a cloud-based system.},
publisher = {Apress : Imprint: Apress},
isbn = {1-4842-3012-4},
year = {2017},
title = {Docker for Data Science : Building Scalable and Extensible Data Infrastructure Around the Jupyter Notebook Server},
language = {eng},
address = {Berkeley, CA},
author = {Cook, Joshua},
keywords = {Big data; Artificial intelligence; Open source software; Computer programming; Python (Computer program language); Big Data; Artificial Intelligence; Open Source; Python},
}

@book{Kinnary2018ADVU,
abstract = {Discover how a software engineer can leverage Docker in order to expedite development velocity. This book focuses on the fundamental concepts this program is built upon and explores how it can help you get your services up and running inside Docker containers. You'll also review tips on how to debug microservices applications that run inside Docker containers. Tech companies are now developing complex softwares that are comprised of multiple services running on different platforms, and Docker has become an essential part of coordinating the communication between these services and platforms. This book addresses problems caused by drifting microservices, debugging across services, inconsistent environments across machines, and coordinating development of machine learning systems between a team of developers, etc. Accelerating Development Velocity Using Docker puts you on the path to transforming your complex systems into more efficient ones.},
publisher = {Apress : Imprint: Apress},
isbn = {1-4842-3936-9},
year = {2018},
title = {Accelerating Development Velocity Using Docker : Docker Across Microservices},
edition = {1st ed. 2018.},
language = {eng},
address = {Berkeley, CA},
author = {Jangla, Kinnary},
keywords = {Open source software; Computer programming; Open Source},
}

@article{ThompsonWilliamR.1933OtLt,
issn = {00063444},
journal = {Biometrika},
pages = {285--294},
volume = {25},
publisher = {Biometrika Office, University College, London},
number = {3/4},
year = {1933},
title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
language = {eng},
author = {Thompson, William R.},
keywords = {Mathematics -- Mathematical procedures -- Approximation ; Mathematics -- Pure mathematics -- Discrete mathematics ; Philosophy -- Logic -- Logical topics ; Mathematics -- Mathematical expressions -- Mathematical functions},
}

