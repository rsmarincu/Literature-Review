
@article{Vanschoren2014,
abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
archivePrefix = {arXiv},
arxivId = {1407.7722},
author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
doi = {10.1145/2641190.2641198},
eprint = {1407.7722},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/2641190.2641198.pdf:pdf},
number = {2},
pages = {49--60},
title = {{OpenML: networked science in machine learning}},
url = {http://arxiv.org/abs/1407.7722{\%}0Ahttp://dx.doi.org/10.1145/2641190.2641198},
volume = {15},
year = {2014}
}
@article{Guyon2015,
abstract = {ChaLearn is organizing the Automatic Machine Learning (AutoML) contest for IJCNN 2015, which challenges participants to solve classification and regression problems without any human intervention. Participants' code is automatically run on the contest servers to train and test learning machines. However, there is no obligation to submit code; half of the prizes can be won by submitting prediction results only. Datasets of progressively increasing difficulty are introduced throughout the six rounds of the challenge. (Participants can enter the competition in any round.) The rounds alternate phases in which learners are tested on datasets participants have not seen, and phases in which participants have limited time to tweak their algorithms on those datasets to improve performance. This challenge will push the state of the art in fully automatic machine learning on a wide range of real-world problems. The platform will remain available beyond the termination of the challenge.},
author = {Guyon, Isabelle and Bennett, Kristin and Cawley, Gavin and Escalante, Hugo Jair and Escalera, Sergio and Ho, Tin Kam and Maci{\`{a}}, N{\'{u}}ria and Ray, Bisakha and Saeed, Mehreen and Statnikov, Alexander and Viegas, Evelyne},
doi = {10.1109/IJCNN.2015.7280767},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/07280767.pdf:pdf},
isbn = {9781479919604},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {Measurement,Reactive power},
pages = {1--8},
publisher = {IEEE},
title = {{Design of the 2015 ChaLearn AutoML challenge}},
volume = {2015-Septe},
year = {2015}
}
@article{Dahl2013,
author = {Dahl, George and Sainath, Tara and Hinton, Geoffrey},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Improving Deep Neural Networks for LVCSR Using Rectified Linear Units and Dropout.pdf:pdf},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
pages = {8609--8613},
title = {{Improving Deep Neural Netowrks for Larfe Vocabulary Continuous Speech Recognition (LVCSR) Using Recitified Linear Units and Dropout, Department of Computer Science , University of Toronto}},
url = {https://ieeexplore.ieee.org/abstract/document/6639346/},
year = {2013}
}
@article{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
doi = {10.1007/978-3-642-25566-3_40},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/10-TR-SMAC.pdf:pdf},
isbn = {9783642255656},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
volume = {6683 LNCS},
year = {2011}
}
@article{Rivolli2018,
abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.},
archivePrefix = {arXiv},
arxivId = {1808.10406},
author = {Rivolli, Adriano and Garcia, Lu{\'{i}}s P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, Andr{\'{e}} C. P. L. F.},
eprint = {1808.10406},
file = {:C$\backslash$:/Users/Robert/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivolli et al. - 2018 - Characterizing classification datasets a study of meta-features for meta-learning.pdf:pdf},
title = {{Characterizing classification datasets: a study of meta-features for meta-learning}},
url = {http://arxiv.org/abs/1808.10406},
year = {2018}
}
@article{Vanschoren,
author = {Vanschoren, Joaquin},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/AutoML{\_}Book{\_}Chapter2.pdf:pdf},
isbn = {9783030053185},
pages = {35--61},
title = {{Meta-Learning}}
}
@article{Picado2018,
abstract = {Given a relational database and training examples for a target relation, relational learning algorithms learn a definition for the target relation in terms of the existing relations in the database. We propose a relational learning system called CastorX, which learns efficiently across multiple heterogeneous databases. The user specifies connections and relationships between different databases using a set of declarative constraints called matching dependencies (MDs). Each MD connects tuples across multiple databases that are related and can meaningfully join but the values of their join attributes may not be equal due to the different representations of these values in different databases. CastorX leverages these constraints during learning to find the information relevant to the training data and target definition across multiple databases. Since each tuple in a database may be connected to too many tuples in other databases according to an MD, the learning process will become very slow. Hence, CastorX uses sampling techniques to learn efficiently and output accurate definitions.},
author = {Picado, J. Jose and Termehchy, A. Arash and Pathak, S. Sudhanshu},
doi = {10.1145/3209889.3209897},
file = {:C$\backslash$:/Users/Robert/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Picado, Termehchy, Pathak - 2018 - Learning efficiently over heterogeneous databases Sampling and constraints to the rescue.pdf:pdf},
isbn = {9781450358286},
journal = {Proceedings of the 2nd Workshop on Data Management for End-To-End Machine Learning, DEEM 2018 - In conjunction with the 2018 ACM SIGMOD/PODS Conference},
pages = {1--4},
title = {{Learning efficiently over heterogeneous databases: Sampling and constraints to the rescue}},
year = {2018}
}
@article{Lake2017,
abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
doi = {10.1017/S0140525X16001837},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/building{\_}machines{\_}that{\_}learn{\_}and{\_}think{\_}like{\_}people.pdf:pdf},
issn = {14691825},
journal = {Behavioral and Brain Sciences},
number = {May},
title = {{Building machines that learn and think like people}},
volume = {40},
year = {2017}
}
@article{Gil2019,
author = {Gil, Yolanda and Honaker, James and Orazio, Vito D and Garijo, Daniel and Jahanshad, Neda},
file = {:D$\backslash$:/Downloads/3301275.3302324.pdf:pdf},
isbn = {9781450362726},
pages = {614--624},
title = {{Towards Human-Guided Machine Learning}},
year = {2019}
}
@article{Abdulrahman2018,
abstract = {Algorithm selection methods can be speeded-up substantially by incorporating multi-objective measures that give preference to algorithms that are both promising and fast to evaluate. In this paper, we introduce such a measure, A3R, and incorporate it into two algorithm selection techniques: average ranking and active testing. Average ranking combines algorithm rankings observed on prior datasets to identify the best algorithms for a new dataset. The aim of the second method is to iteratively select algorithms to be tested on the new dataset, learning from each new evaluation to intelligently select the next best candidate. We show how both methods can be upgraded to incorporate a multi-objective measure A3R that combines accuracy and runtime. It is necessary to establish the correct balance between accuracy and runtime, as otherwise time will be wasted by conducting less informative tests. The correct balance can be set by an appropriate parameter setting within function A3R that trades off accuracy and runtime. Our results demonstrate that the upgraded versions of Average Ranking and Active Testing lead to much better mean interval loss values than their accuracy-based counterparts.},
author = {Abdulrahman, Salisu Mamman and Brazdil, Pavel and van Rijn, Jan N. and Vanschoren, Joaquin},
doi = {10.1007/s10994-017-5687-8},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Abdulrahman2018{\_}Article{\_}SpeedingUpAlgorithmSelectionUs.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Active testing,Algorithm selection,Average ranking,Loss curves,Mean interval loss,Meta-learning,Ranking of algorithms},
number = {1},
pages = {79--108},
publisher = {Springer US},
title = {{Speeding up algorithm selection using average ranking and active testing by introducing runtime}},
volume = {107},
year = {2018}
}
@article{Bilalli2017,
abstract = {The demand for performing data analysis is steadily rising. As a consequence, people of different profiles (i.e., nonexperienced users) have started to analyze their data. However, this is challenging for them. A key step that poses difficulties and determines the success of the analysis is data mining (model/algorithm selection problem). Meta-learning is a technique used for assisting non-expert users in this step. The effectiveness of meta-learning is, however, largely dependent on the description/characterization of datasets (i.e., meta-features used for meta-learning). There is a need for improving the effectiveness of meta-learning by identifying and designing more predictive meta-features. In this work, we use a method from exploratory factor analysis to study the predictive power of different meta-features collected in OpenML, which is a collaborative machine learning platform that is designed to store and organize meta-data about datasets, data mining algorithms, models and their evaluations. We first use the method to extract latent features, which are abstract concepts that group together meta-features with common characteristics. Then, we study and visualize the relationship of the latent features with three different performance measures of four classification algorithms on hundreds of datasets available in OpenML, and we select the latent features with the highest predictive power. Finally, we use the selected latent features to perform meta-learning and we show that our method improves the meta-learning process. Furthermore, we design an easy to use application for retrieving different meta-data from OpenML as the biggest source of data in this domain.},
author = {Bilalli, Besim and Abell{\'{o}}, Alberto and Aluja-Banet, Tom{\`{a}}s},
doi = {10.1515/amcs-2017-0048},
file = {:D$\backslash$:/Downloads/[20838492 - International Journal of Applied Mathematics and Computer Science] On the predictive power of meta-features in OpenML.pdf:pdf},
issn = {20838492},
journal = {International Journal of Applied Mathematics and Computer Science},
keywords = {feature extraction,feature selection,meta-learning},
number = {4},
pages = {697--712},
title = {{On the predictive power of meta-features in OpenML}},
volume = {27},
year = {2017}
}
@book{Feurer2019,
abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
author = {Feurer, Matthias and Hutter, Frank},
doi = {10.1007/978-3-030-05318-5_1},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/AutoML{\_}Book{\_}Chapter1.pdf:pdf},
isbn = {9783030053185},
pages = {3--33},
title = {{Hyperparameter Optimization}},
year = {2019}
}
@article{Mohr2018,
abstract = {Automated machine learning (AutoML) seeks to automatically select, compose, and parametrize machine learning algorithms, so as to achieve optimal performance on a given task (dataset). Although current approaches to AutoML have already produced impressive results, the field is still far from mature, and new techniques are still being developed. In this paper, we present ML-Plan, a new approach to AutoML based on hierarchical planning. To highlight the potential of this approach, we compare ML-Plan to the state-of-the-art frameworks Auto-WEKA, auto-sklearn, and TPOT. In an extensive series of experiments, we show that ML-Plan is highly competitive and often outperforms existing approaches.},
author = {Mohr, Felix and Wever, Marcel and H{\"{u}}llermeier, Eyke},
doi = {10.1007/s10994-018-5735-z},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Mohr2018{\_}Article{\_}ML-PlanAutomatedMachineLearnin.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Algorithm configuration,Algorithm selection,Automated machine learning,Automated planning,Heuristic search},
number = {8-10},
pages = {1495--1515},
publisher = {Springer US},
title = {{ML-Plan: Automated machine learning via hierarchical planning}},
url = {https://doi.org/10.1007/s10994-018-5735-z},
volume = {107},
year = {2018}
}
@article{Komer2014,
abstract = {Hyperopt-sklearn is a new software project that provides automatic algorithm configuration of the Scikit-learn machine learning library. Following Auto-Weka, we take the view that the choice of classifier and even the choice of pre-processing module can be taken together to represent a single large hyperparameter optimization problem. We use Hyperopt to define a search space that encompasses many standard components (e.g. SVM, RF, KNN, PCA, TFIDF) and common patterns of composing them together. We demonstrate, using search algorithms in Hyperopt and standard benchmarking data sets (MNIST, 20-Newsgroups, Convex Shapes), that searching this space is practical and effective. In particular, we improve on best-known scores for the model space for both MNIST and Convex Shapes.},
author = {Komer, Brent and Bergstra, James and Eliasmith, Chris},
doi = {10.25080/majora-14bd3278-006},
file = {:D$\backslash$:/Downloads/Paper19.pdf:pdf},
journal = {Proceedings of the 13th Python in Science Conference},
pages = {32--37},
title = {{Hyperopt-Sklearn: Automatic Hyperparameter Configuration for Scikit-Learn}},
year = {2014}
}
@article{Li2019,
abstract = {Customer service is transforming from traditional manual service toward automated service, which utilizes different computational informatics to achieve a higher efficient and quality services. Automated customer service requires big data and expertise in data analysis as prerequisites. However, many companies, especially small and medium enterprises, do not have sufficient data and experience due to their limited scale and resources. They need to rely on third parties, and this reliance results in the lack of development of core customer service competency. In order to overcome these challenges, an open and automated customer service platform based on Internet of things (IoT), blockchain, and automated machine learning (AutoML) is proposed. The data are gathered with the use of IoT devices during the customer service. An open but secured environment to achieve data trading is ensured by using blockchain. AutoML is adopted to automate the data analysis processes for reducing the reliance of costly experts. The proposed platform is analyzed through use case evaluation. A prototype system has also been developed and evaluated. The simulation results show that our platform is scalable and efficient.},
author = {Li, Zhi and Guo, Hanyang and Wang, Wai Ming and Guan, Yijiang and Barenji, Ali Vatankhah and Huang, George Q. and McFall, Kevin S. and Chen, Xin},
doi = {10.1109/TII.2019.2900987},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/08649758.pdf:pdf},
issn = {19410050},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {Automated customer service,automated machine learning (AutoML),blockchain,open customer service},
number = {6},
pages = {3642--3651},
publisher = {IEEE},
title = {{A blockchain and automl approach for open and automated customer service}},
volume = {15},
year = {2019}
}
@article{Feurer2015,
abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.},
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/5872-efficient-and-robust-automated-machine-learning.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2962--2970},
title = {{Efficient and robust automated machine learning}},
volume = {2015-Janua},
year = {2015}
}
@article{Jin2019,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet [51], PNAS [29], usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.10282v3},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
doi = {10.1145/3292500.3330648},
eprint = {arXiv:1806.10282v3},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/1806.10282.pdf:pdf},
isbn = {9781450362016},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {AutoML,Automated Machine Learning,Bayesian Optimization,Network Morphism,Neural Architecture Search},
pages = {1946--1956},
title = {{Auto-keras: An efficient neural architecture search system}},
year = {2019}
}
@article{Baydin2014,
abstract = {Automatic differentiation---the mechanical transformation of numeric computer programs to calculate derivatives efficiently and accurately---dates to the origin of the computer age. Reverse mode automatic differentiation both antedates and generalizes the method of backwards propagation of errors used in machine learning. Despite this, practitioners in a variety of fields, including machine learning, have been little influenced by automatic differentiation, and make scant use of available tools. Here we review the technique of automatic differentiation, describe its two main modes, and explain how it can benefit machine learning practitioners. To reach the widest possible audience our treatment assumes only elementary differential calculus, and does not assume any knowledge of linear algebra.},
archivePrefix = {arXiv},
arxivId = {1404.7456},
author = {Baydin, Atilim Gunes and Pearlmutter, Barak A.},
eprint = {1404.7456},
file = {:D$\backslash$:/Downloads/Paper 4.pdf:pdf},
keywords = {automatic differentiation,machine learning,optimization},
pages = {1--7},
title = {{Automatic Differentiation of Algorithms for Machine Learning}},
url = {http://arxiv.org/abs/1404.7456},
year = {2014}
}
@article{Shang2019a,
abstract = {Statistical knowledge and domain expertise are key to extract actionable insights out of data, yet such skills rarely coexist together. In Machine Learning, high-quality results are only attainable via mindful data preprocessing, hyperparameter tuning and model selection. Domain experts are often overwhelmed by such complexity, de-facto inhibiting a wider adoption of ML techniques in other elds. Existing libraries that claim to solve this problem, still require well-trained practitioners. Those frameworks involve heavy data preparation steps and are often too slow for interactive feedback from the user, severely limiting the scope of such systems. In this paper we present Alpine Meadow, arst Interactive Automated Machine Learning tool. What makes our system unique is not only the focus on interactivity, but also the combined systemic and algorithmic design approach; on one hand we leverage ideas from query optimization, on the other we devise novel selection and pruning strategies combining cost-based Multi-Armed Bandits and Bayesian Optimization. We evaluate our system on over 300 datasets and compare against other AutoML tools, including the current NIPS winner, as well as expert solutions. Not only is Alpine Meadow able to signicantly outperform the other AutoML systems while - in contrast to the other systems - providing interactive latencies, but also outperforms in 80{\%} of the cases expert solutions over data sets we have never seen before.},
author = {Shang, Zeyuan and Zgraggen, Emanuel and Buratti, Benedetto and Kossmann, Ferdinand and Eichmann, Philipp and Chung, Yeounoh and Binnig, Carsten and Upfal, Eli and Kraska, Tim},
doi = {10.1145/3299869.3319863},
file = {:C$\backslash$:/Users/Robert/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shang et al. - 2019 - Democratizing data science through interactive curation of ML pipelines.pdf:pdf},
isbn = {9781450356435},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
pages = {1171--1188},
title = {{Democratizing data science through interactive curation of ML pipelines}},
year = {2019}
}
@article{VanRijn2013,
abstract = {We present OpenML, a novel open science platform that provides easy access to machine learning data, software and results to encourage further study and application. It organizes all submitted results online so they can be easily found and reused, and features a web API which is being integrated in popular machine learning tools such as Weka, KNIME, RapidMiner and R packages, so that experiments can be shared easily. {\textcopyright} 2013 Springer-Verlag.},
author = {{Van Rijn}, Jan N. and Bischl, Bernd and Torgo, Luis and Gao, Bo and Umaashankar, Venkatesh and Fischer, Simon and Winter, Patrick and Wiswedel, Bernd and Berthold, Michael R. and Vanschoren, Joaquin},
doi = {10.1007/978-3-642-40994-3_46},
file = {:D$\backslash$:/Downloads/OpenML - A Collaborative Science Platform.pdf:pdf},
isbn = {9783642409936},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Databases,Experimental Methodology,Machine Learning,Meta-Learning},
number = {PART 3},
pages = {645--649},
title = {{OpenML: A collaborative science platform}},
volume = {8190 LNAI},
year = {2013}
}
@article{Thornton2013,
abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
archivePrefix = {arXiv},
arxivId = {1208.3719},
author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
doi = {10.1145/2487575.2487629},
eprint = {1208.3719},
file = {:D$\backslash$:/Downloads/2487575.2487629.pdf:pdf},
isbn = {9781450321747},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Hyperparameter optimization,Model selection,Weka},
pages = {847--855},
title = {{Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms}},
volume = {Part F1288},
year = {2013}
}
@article{Kotthoff2017,
abstract = {WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.},
author = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
doi = {10.1007/978-3-030-05318-5_4},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/AutoML{\_}Book{\_}Chapter4.pdf:pdf},
isbn = {9783030053185},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Feature Selection,Hyperparameter Optimization,Model Selection},
pages = {1--5},
title = {{Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA}},
volume = {18},
year = {2017}
}
@article{Feurer2015a,
abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a sub-community of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem.},
author = {Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/initializing bayesian hyperparameter optimization via Meta-Learning matthias feurer.pdf:pdf},
isbn = {9781577357001},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Heuristic Search and Optimization Track},
pages = {1128--1135},
title = {{Initializing Bayesian hyperparameter optimization via meta-learning}},
volume = {2},
year = {2015}
}
