@article{Swearingen2017,
abstract = {In this paper, we present Auto-Tuned Models, or ATM, a distributed, collaborative, scalable system for automated machine learning. Users of ATM can simply upload a dataset, choose a subset of modeling methods, and choose to use ATM's hybrid Bayesian and multi-armed bandit optimization system. The distributed system works in a load-balanced fashion to quickly deliver results in the form of ready-to-predict models, confusion matrices, cross-validation results, and training timings. By automating hyperparameter tuning and model selection, ATM returns the emphasis of the machine learning workflow to its most irreducible part: feature engineering. We demonstrate the usefulness of ATM on 420 datasets from OpenML and train over 3 million classifiers. Our initial results show ATM can beat human-generated solutions for 30{\%} of the datasets, and can do so in 1/100th of the time.},
author = {Swearingen, Thomas and Drevo, Will and Cyphers, Bennett and Cuesta-Infante, Alfredo and Ross, Arun and Veeramachaneni, Kalyan},
doi = {10.1109/BigData.2017.8257923},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/08257923.pdf:pdf},
isbn = {9781538627143},
journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
pages = {151--162},
title = {{ATM: A distributed, collaborative, scalable system for automated machine learning}},
volume = {2018-Janua},
year = {2017}
}
@article{Vergilio2018,
abstract = {This paper presents a contribution to the fields of Big Data Analytics and Software Architecture, namely an emerging and unifying architectural pattern for big data processing in the cloud from a cloud consumer's perspective. PaaS-BDP (Platform-as-a-Service for Big Data) is an architectural pattern based on resource pooling and the use of a unified programming model for building big data processing pipelines capable of processing both batch and stream data. It uses container cluster technology on a PaaS service model to overcome common shortfalls of current big data solutions offered by major cloud providers such as low portability, lack of interoperability and the risk of vendor lock-in.},
author = {Vergilio, Thalita and Ramachandran, Muthu},
doi = {10.5220/0006632400450052},
file = {:D$\backslash$:/Downloads/COMPLEXIS{\_}2018{\_}1{\_}CR.pdf:pdf},
isbn = {9789897582974},
journal = {COMPLEXIS 2018 - Proceedings of the 3rd International Conference on Complexity, Future Information Systems and Risk},
keywords = {Big Data,Containers,Docker Swarm,Multi-Cloud,Orchestration,PaaS,Resource Pooling},
number = {March},
pages = {45--52},
title = {{PaaS-BDP a multi-cloud architectural pattern for big data processing on a platform-as-a-service model}},
volume = {2018-March},
year = {2018}
}
@article{Bubeck2012,
abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model. {\textcopyright} 2012 S. Bubeck and N. Cesa-Bianchi.},
archivePrefix = {arXiv},
arxivId = {1204.5721},
author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}}},
doi = {10.1561/2200000024},
eprint = {1204.5721},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/1204.5721.pdf:pdf},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--122},
title = {{Regret analysis of stochastic and nonstochastic multi-armed bandit problems}},
volume = {5},
year = {2012}
}
@article{Audibert2010,
abstract = {We consider the problem of finding the best arm in a stochastic multi-armed bandit game. The regret of a forecaster is here defined by the gap between the mean reward of the optimal arm and the mean reward of the ultimately chosen arm. We propose a highly exploring UCB policy and a new algorithm based on successive rejects. We show that these algorithms are essentially optimal since their regret decreases exponentially at a rate which is, up to a logarithmic factor, the best possible. However, while the UCB policy needs the tuning of a parameter depending on the unobservable hardness of the task, the successive rejects policy benefits from being parameter-free, and also independent of the scaling of the rewards. As a by-product of our analysis, we show that identifying the best arm (when it is unique) requires a number of samples of order (up to a log(K) factor) $\Sigma$i 1/$\Delta$2, where the sum is on the suboptimal arms and $\Delta$i represents the difference between the mean reward of the best arm and the one of arm i. This generalizes the well-known fact that one needs of order of 1/$\Delta$2 samples to differentiate the means of two distributions with gap $\Delta$.},
author = {Audibert, Jean Yves and Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/COLT10.pdf:pdf},
isbn = {9780982252925},
journal = {COLT 2010 - The 23rd Conference on Learning Theory},
pages = {41--53},
title = {{Best arm identification in multi-armed bandits}},
year = {2010}
}
@article{Schmidt2020,
abstract = {Automated algorithm selection and hyperparameter tuning facilitates the application of machine learning. Traditional multi-armed bandit strategies look to the history of observed rewards to identify the most promising arms for optimizing expected total reward in the long run. When considering limited time budgets and computational resources, this backward view of rewards is inappropriate as the bandit should look into the future for anticipating the highest final reward at the end of a specified time budget. This work addresses that insight by introducing HAMLET, which extends the bandit approach with learning curve extrapolation and computation time-awareness for selecting among a set of machine learning algorithms. Results show that the HAMLET Variants 1-3 exhibit equal or better performance than other bandit-based algorithm selection strategies in experiments with recorded hyperparameter tuning traces for the majority of considered time budgets. The best performing HAMLET Variant 3 combines learning curve extrapolation with the well-known upper confidence bound exploration bonus. That variant performs better than all non-HAMLET policies with statistical significance at the 95{\%} level for 1,485 runs.},
archivePrefix = {arXiv},
arxivId = {2001.11261},
author = {Schmidt, Mischa and Gastinger, Julia and Nicolas, S{\'{e}}bastien and Sch{\"{u}}lke, Anett},
eprint = {2001.11261},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/2001.11261.pdf:pdf},
title = {{HAMLET -- A Learning Curve-Enabled Multi-Armed Bandit for Algorithm Selection}},
url = {http://arxiv.org/abs/2001.11261},
year = {2020}
}
@article{Zhou2019,
abstract = {Traditionally vehicles act only as servers in transporting passengers and goods. With increasing sensor equipment in vehicles, including automated vehicles, there is a need to test algorithms that consider the dual role of vehicles as both servers and sensors. The paper formulates a sequential route selection problem as a shortest path problem with on-time arrival reliability under a multi-armed bandit setting, a type of reinforcement learning model. A decision-maker has to make a finite set of decisions sequentially on departure time and path between a fixed origin-destination pair such that on-time reliability is maximized while travel time is minimized. The upper confidence bound algorithm is extended to handle this problem. Several tests are conducted. First, simulated data successfully verifies the method, then a real-data scenario is constructed of a hotel shuttle service from midtown Manhattan in New York City providing hourly access to John F. Kennedy International Airport. Results suggest that route selection with multi-armed bandit learning algorithms can be effective but neglecting passenger scheduling constraints can have negative effects on on-time arrival reliability by as much as 4.8{\%} and combined reliability and travel time by 66.1{\%}.},
author = {Zhou, Jinkai and Lai, Xuebo and Chow, Joseph Y.J.},
doi = {10.1177/0361198119850457},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Multi-Armed Bandit On-Time Arrival Algorithms for Sequential Reliable Route Selection under Uncertainty.pdf:pdf},
issn = {21694052},
journal = {Transportation Research Record},
number = {10},
pages = {673--682},
title = {{Multi-Armed Bandit On-Time Arrival Algorithms for Sequential Reliable Route Selection under Uncertainty}},
volume = {2673},
year = {2019}
}
@article{Denardo2013,
abstract = {Presented in this paper is a self-contained analysis of a Markov decision problem that is known as the multi-armed bandit. The analysis covers the cases of linear and exponential utility functions. The optimal policy is shown to have a simple and easily-implemented form. Procedures for computing such a policy are presented, as are procedures for computing the expected utility that it earns, given any starting state. For the case of linear utility, constraints that link the bandits are introduced, and the constrained optimization problem is solved via column generation. The methodology is novel in several respects, which include the use of elementary row operations to simplify arguments. {\textcopyright} 2012 Springer Science+Business Media New York.},
archivePrefix = {arXiv},
arxivId = {1203.4640},
author = {Denardo, Eric V. and Feinberg, Eugene A. and Rothblum, Uriel G.},
doi = {10.1007/s10479-012-1250-y},
eprint = {1203.4640},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Denardo2013{\_}Article{\_}TheMulti-armedBanditWithConstr.pdf:pdf},
issn = {02545330},
journal = {Annals of Operations Research},
number = {1},
pages = {37--62},
title = {{The multi-armed bandit, with constraints}},
volume = {208},
year = {2013}
}
@article{DasDores2018,
abstract = {Machine Learning (ML) has been successfully applied to a wide range of domains and applications. Since the number of ML applications is growing, there is a need for tools that boost the data scientist's productivity. Automated Machine Learning (AutoML) is the field of ML that aims to address these needs through the development of solutions which enable data science practitioners, experts and non-experts, to efficiently create fine-tuned predictive models with minimum intervention. In this paper, we present the application of the multi-armed bandit optimization algorithm Hyperband to address the AutoML problem of generating customized classification workflows, a combination of preprocessing methods and ML algorithms including hyperparameter optimization. Experimental results comparing the bandit-based approach against Auto ML Bayesian Optimization methods show that this new approach is superior to the state-of-the-art methods in the test evaluation and equivalent to them in a statistical analysis.},
author = {{Das Dores}, Silvia Cristina Nunes and Soares, Carlos and Ruiz, Duncan},
doi = {10.1109/BRACIS.2018.00029},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Bandit-Based Automated Machine Learning.pdf:pdf},
isbn = {9781538680230},
journal = {Proceedings - 2018 Brazilian Conference on Intelligent Systems, BRACIS 2018},
keywords = {autoband,automl,machine learning,workflow selection},
pages = {121--126},
title = {{Bandit-based automated machine learning}},
year = {2018}
}
@article{Kamiura2017,
abstract = {The principle of optimism in the face of uncertainty is known as a heuristic in sequential decision-making problems. Overtaking method based on this principle is an effective algorithm to solve multi-armed bandit problems. It was defined by a set of some heuristic patterns of the formulation in the previous study. The objective of the present paper is to redefine the value functions of Overtaking method and to unify the formulation of them. The unified Overtaking method is associated with upper bounds of confidence intervals of expected rewards on statistics. The unification of the formulation enhances the universality of Overtaking method. Consequently we newly obtain Overtaking method for the exponentially distributed rewards, numerically analyze it, and show that it outperforms UCB algorithm on average. The present study suggests that the principle of optimism in the face of uncertainty should be regarded as the statistics-based consequence of the law of large numbers for the sample mean of rewards and estimation of upper bounds of expected rewards, rather than as a heuristic, in the context of multi-armed bandit problems.},
author = {Kamiura, Moto and Sano, Kohei},
doi = {10.1016/j.biosystems.2017.08.004},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/1-s2.0-S030326471730268X-main.pdf:pdf},
issn = {18728324},
journal = {BioSystems},
keywords = {Confidence interval,Multi-armed bandit problem,The principle of optimism in the face of uncertain,UCB algorithm},
pages = {25--32},
publisher = {Elsevier Ireland Ltd},
title = {{Optimism in the face of uncertainty supported by a statistically-designed multi-armed bandit algorithm}},
url = {http://dx.doi.org/10.1016/j.biosystems.2017.08.004},
volume = {160},
year = {2017}
}
@article{Slivkins2019,
abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments.},
archivePrefix = {arXiv},
arxivId = {1904.07272},
author = {Slivkins, Aleksandrs},
doi = {10.1561/2200000068},
eprint = {1904.07272},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/1904.07272.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
number = {1-2},
pages = {1--286},
title = {{Introduction to multi-armed bandits}},
volume = {12},
year = {2019}
}
@article{Zhang2019,
abstract = {Monte Carlo (MC) permutation test is considered the gold standard for statistical hypothesis testing, especially when standard parametric assumptions arc not clear or likely to fail. However, in modern data science settings where a large number of hypothesis tests need to be performed simultaneously, it is rarely used due to its prohibitive computational cost. In genome-wide association studies, for example, the number of hypothesis tests m is around 106 while the number of MC samples n for each test could be greater than 108, totaling more than nm=1014 samples. In this paper, we propose Adaptive MC multiple Testing (AMT) to estimate MC p-values and control false discovery rate in multiple testing. The algorithm outputs the same result as the standard full MC approach with high probability while requiring only {\~{O}}(√nm)samples. This sample complexity is shown to be optimal. On a Parkinson GWAS dataset, the algorithm reduces the running time from 2 months for full MC to an hour. The AMT algorithm is derived based on the theory of multi-armed bandits.},
archivePrefix = {arXiv},
arxivId = {1902.00197},
author = {Zhang, Martin J. and Zou, James and Tse, David},
eprint = {1902.00197},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/Adaptive Monte Carlo multiple testing via multi-armed bandit.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
number = {Mc},
pages = {12967--12984},
title = {{Adaptive Monte Carlo multiple testing via multi-armed bandits}},
volume = {2019-June},
year = {2019}
}
@article{Li2018,
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
archivePrefix = {arXiv},
arxivId = {1603.06560},
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
eprint = {1603.06560},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/1603.06560.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Hyperparameter optimization,Infinite-armed bandits,Model selection,Online optimization},
pages = {1--52},
title = {{Hyperband: A novel bandit-based approach to hyperparameter optimization}},
volume = {18},
year = {2018}
}
@article{Amirizadeh2015,
author = {Amirizadeh, Khosrow and Mandava, Rajeswari},
doi = {10.17706/jsw.10.3.239-249},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/21-jsw140215-1118.pdf:pdf},
issn = {1796217X},
journal = {Journal of Software},
keywords = {adaptive incremental learning,enhanced mab,mab empirical evaluations,setting-free step-},
number = {3},
pages = {239--249},
title = {{Accelerated -Greedy Multi Armed Bandit Algorithm for Online Sequential-Selections Applications}},
volume = {10},
year = {2015}
}
@article{Dai2011,
abstract = {In the classic Bayesian restless multi-armed bandit (RMAB) problem, there are N arms, with rewards on all arms evolving at each time as Markov chains with known parameters. A player seeks to activate K ≥ 1 arms at each time in order to maximize the expected total reward obtained over multiple plays. RMAB is a challenging problem that is known to be PSPACE-hard in general. We consider in this work the even harder non-Bayesian RMAB, in which the parameters of the Markov chain are assumed to be unknown a priori. We develop an original approach to this problem that is applicable when the corresponding Bayesian problem has the structure that, depending on the known parameter values, the optimal solution is one of a prescribed finite set of policies. In such settings, we propose to learn the optimal policy for the non-Bayesian RMAB by employing a suitable meta-policy which treats each policy from this finite set as an arm in a different non-Bayesian multi-armed bandit problem for which a single-arm selection policy is optimal. We demonstrate this approach by developing a novel sensing policy for opportunistic spectrum access over unknown dynamic channels. We prove that our policy achieves near-logarithmic regret (the difference in expected reward compared to a model-aware genie), which leads to the same average reward that can be achieved by the optimal policy under a known model. This is the first such result in the literature for a non-Bayesian RMAB. {\textcopyright} 2011 IEEE.},
author = {Dai, Wenhan and Gai, Yi and Krishnamachari, Bhaskar and Zhao, Qing},
doi = {10.1109/ICASSP.2011.5946273},
file = {:D$\backslash$:/Documents/Bath/Dissertation/References/THE NON-BAYESIAN RESTLESS MULTI-ARMED BANDIT.pdf:pdf},
isbn = {9781457705397},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {learning,non-Bayesian,opportunistic spectrum access,regret,restless bandit},
pages = {2940--2943},
publisher = {IEEE},
title = {{The non-Bayesian restless multi-armed bandit: A case of near-logarithmic regret}},
year = {2011}
}
